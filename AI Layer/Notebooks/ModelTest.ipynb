{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb2dbda",
   "metadata": {},
   "source": [
    "# üöó Safespace Model Testing Notebook\n",
    "\n",
    "This notebook allows you to test the accident detection model with:\n",
    "- **Video files** - Test with pre-recorded footage\n",
    "- **Camera feed** - Real-time detection from webcam\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "969a8b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:119.)\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1173d",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdb073e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully from: ../Models/Car Accident.pt\n",
      "   Classes: {0: 'accident', 1: 'person', 2: 'vehicle'}\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = \"../Models/Car Accident.pt\"  # Path to your model\n",
    "CONFIDENCE_THRESHOLD = 0.5  # Detection confidence threshold\n",
    "\n",
    "# Load the model\n",
    "model = YOLO(MODEL_PATH)\n",
    "model.to(device)\n",
    "print(f\"‚úÖ Model loaded successfully from: {MODEL_PATH}\")\n",
    "print(f\"   Classes: {model.names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa66091",
   "metadata": {},
   "source": [
    "## Detection Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1229e1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def detect_and_annotate(frame, model, confidence=0.5):\n",
    "    \"\"\"\n",
    "    Run detection on a frame and return annotated frame with detections.\n",
    "    \n",
    "    Args:\n",
    "        frame: Input BGR frame\n",
    "        model: YOLO model\n",
    "        confidence: Detection confidence threshold\n",
    "        \n",
    "    Returns:\n",
    "        annotated_frame: Frame with bounding boxes\n",
    "        detections: Supervision Detections object\n",
    "        inference_time: Time taken for inference in ms\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run inference\n",
    "    results = model.predict(frame, conf=confidence, device=device, verbose=False)\n",
    "    \n",
    "    inference_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "    \n",
    "    # Convert to supervision detections\n",
    "    detections = sv.Detections.from_ultralytics(results[0])\n",
    "    \n",
    "    # Create annotators\n",
    "    box_annotator = sv.BoxAnnotator(\n",
    "        thickness=2,\n",
    "    )\n",
    "    label_annotator = sv.LabelAnnotator(\n",
    "        text_scale=0.5,\n",
    "        text_thickness=1,\n",
    "    )\n",
    "    \n",
    "    # Generate labels\n",
    "    labels = [\n",
    "        f\"{model.names[class_id]} {confidence:.2f}\"\n",
    "        for class_id, confidence in zip(detections.class_id, detections.confidence)\n",
    "    ] if len(detections) > 0 else []\n",
    "    \n",
    "    # Annotate frame\n",
    "    annotated_frame = box_annotator.annotate(scene=frame.copy(), detections=detections)\n",
    "    annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
    "    \n",
    "    return annotated_frame, detections, inference_time\n",
    "\n",
    "\n",
    "def add_stats_overlay(frame, fps, inference_time, detection_count):\n",
    "    \"\"\"Add statistics overlay to frame.\"\"\"\n",
    "    overlay = frame.copy()\n",
    "    \n",
    "    # Add semi-transparent background\n",
    "    cv2.rectangle(overlay, (10, 10), (250, 100), (0, 0, 0), -1)\n",
    "    cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)\n",
    "    \n",
    "    # Add text\n",
    "    cv2.putText(frame, f\"FPS: {fps:.1f}\", (20, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Inference: {inference_time:.1f}ms\", (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Detections: {detection_count}\", (20, 85), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a795c5",
   "metadata": {},
   "source": [
    "---\n",
    "## Option 1: Test with Video File üé¨\n",
    "\n",
    "Run the cell below to test the model on a video file. Press **'q'** to quit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4629c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Video loaded: /home/ziadmoh/UNI/Safespace_node/assets/Video Test/test5.mp4\n",
      "   Resolution: 360x640\n",
      "   FPS: 30.0\n",
      "   Total Frames: 315\n",
      "\n",
      "üéÆ Controls: Press 'q' to quit, 'p' to pause/resume, 's' to save frame\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_436581/3340360750.py\", line 36, in <module>\n",
      "    annotated_frame, detections, inference_time = detect_and_annotate(\n",
      "                                                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_436581/4254842974.py\", line 18, in detect_and_annotate\n",
      "    results = model.predict(frame, conf=confidence, device=device, verbose=False)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/ultralytics/engine/model.py\", line 536, in predict\n",
      "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
      "                                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/ultralytics/engine/predictor.py\", line 225, in __call__\n",
      "    return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Results into one\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 38, in generator_context\n",
      "    response = gen.send(None)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/ultralytics/engine/predictor.py\", line 330, in stream_inference\n",
      "    preds = self.inference(im, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/ultralytics/engine/predictor.py\", line 182, in inference\n",
      "    return self.model(im, augment=self.args.augment, visualize=visualize, embed=self.args.embed, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/ultralytics/nn/autobackend.py\", line 701, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize, embed=embed, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/ultralytics/nn/tasks.py\", line 142, in forward\n",
      "    return self.predict(x, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/ultralytics/nn/tasks.py\", line 159, in predict\n",
      "    return self._predict_once(x, profile, visualize, embed)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/ultralytics/nn/tasks.py\", line None, in _predict_once\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2194, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1185, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1056, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 864, in structured_traceback\n",
      "    formatted_exceptions: list[list[str]] = self.format_exception_as_a_whole(\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 776, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ziadmoh/Programs/anaconda3/envs/AI_ENV/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 536, in format_record\n",
      "    assert isinstance(frame_info.lineno, int)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# üé¨ VIDEO FILE TESTING\n",
    "# ========================================\n",
    "\n",
    "# Set your video path here\n",
    "VIDEO_PATH = \"/home/ziadmoh/UNI/Safespace_node/assets/Video Test/test5.mp4\"  # Change this to your video path\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"‚ùå Error: Could not open video file: {VIDEO_PATH}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Video loaded: {VIDEO_PATH}\")\n",
    "    print(f\"   Resolution: {int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))}x{int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))}\")\n",
    "    print(f\"   FPS: {cap.get(cv2.CAP_PROP_FPS):.1f}\")\n",
    "    print(f\"   Total Frames: {int(cap.get(cv2.CAP_PROP_FRAME_COUNT))}\")\n",
    "    print(\"\\nüéÆ Controls: Press 'q' to quit, 'p' to pause/resume, 's' to save frame\")\n",
    "    \n",
    "    paused = False\n",
    "    frame_count = 0\n",
    "    fps_start_time = time.time()\n",
    "    fps = 0\n",
    "    \n",
    "    while True:\n",
    "        if not paused:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                # Loop video\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "                continue\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # Run detection\n",
    "            annotated_frame, detections, inference_time = detect_and_annotate(\n",
    "                frame, model, CONFIDENCE_THRESHOLD\n",
    "            )\n",
    "            \n",
    "            # Calculate FPS\n",
    "            if frame_count % 10 == 0:\n",
    "                fps = 10 / (time.time() - fps_start_time)\n",
    "                fps_start_time = time.time()\n",
    "            \n",
    "            # Add stats overlay\n",
    "            annotated_frame = add_stats_overlay(\n",
    "                annotated_frame, fps, inference_time, len(detections)\n",
    "            )\n",
    "            \n",
    "            # Show alert if accident detected\n",
    "            if len(detections) > 0:\n",
    "                cv2.putText(annotated_frame, \"‚ö†Ô∏è ACCIDENT DETECTED!\", \n",
    "                           (annotated_frame.shape[1]//2 - 200, 50), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)\n",
    "            \n",
    "            cv2.imshow(\"Safespace Model Test - Video\", annotated_frame)\n",
    "        \n",
    "        # Handle key presses\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('p'):\n",
    "            paused = not paused\n",
    "            print(\"‚è∏Ô∏è Paused\" if paused else \"‚ñ∂Ô∏è Resumed\")\n",
    "        elif key == ord('s'):\n",
    "            save_path = f\"detection_frame_{frame_count}.jpg\"\n",
    "            cv2.imwrite(save_path, annotated_frame)\n",
    "            print(f\"üíæ Frame saved: {save_path}\")\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"\\n‚úÖ Video testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e0c2e5",
   "metadata": {},
   "source": [
    "---\n",
    "## Option 2: Test with Camera Feed üì∑\n",
    "\n",
    "Run the cell below to test the model with your webcam. Press **'q'** to quit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "146c10d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Camera opened successfully\n",
      "   Resolution: 1280x720\n",
      "\n",
      "üéÆ Controls: Press 'q' to quit, 's' to save frame\n",
      "\n",
      "‚úÖ Camera testing completed!\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# üì∑ CAMERA FEED TESTING\n",
    "# ========================================\n",
    "\n",
    "# Camera settings\n",
    "CAMERA_INDEX = 0  # Change if you have multiple cameras\n",
    "CAMERA_WIDTH = 1280\n",
    "CAMERA_HEIGHT = 720\n",
    "\n",
    "# Open camera\n",
    "cap = cv2.VideoCapture(CAMERA_INDEX)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, CAMERA_WIDTH)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, CAMERA_HEIGHT)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"‚ùå Error: Could not open camera (index: {CAMERA_INDEX})\")\n",
    "else:\n",
    "    actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    print(f\"‚úÖ Camera opened successfully\")\n",
    "    print(f\"   Resolution: {actual_width}x{actual_height}\")\n",
    "    print(\"\\nüéÆ Controls: Press 'q' to quit, 's' to save frame\")\n",
    "    \n",
    "    frame_count = 0\n",
    "    fps_start_time = time.time()\n",
    "    fps = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"‚ùå Failed to grab frame\")\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        # Run detection\n",
    "        annotated_frame, detections, inference_time = detect_and_annotate(\n",
    "            frame, model, CONFIDENCE_THRESHOLD\n",
    "        )\n",
    "        \n",
    "        # Calculate FPS\n",
    "        if frame_count % 10 == 0:\n",
    "            fps = 10 / (time.time() - fps_start_time)\n",
    "            fps_start_time = time.time()\n",
    "        \n",
    "        # Add stats overlay\n",
    "        annotated_frame = add_stats_overlay(\n",
    "            annotated_frame, fps, inference_time, len(detections)\n",
    "        )\n",
    "        \n",
    "        # Show alert if accident detected\n",
    "        if len(detections) > 0:\n",
    "            cv2.putText(annotated_frame, \"‚ö†Ô∏è ACCIDENT DETECTED!\", \n",
    "                       (annotated_frame.shape[1]//2 - 200, 50), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)\n",
    "        \n",
    "        cv2.imshow(\"Safespace Model Test - Camera\", annotated_frame)\n",
    "        \n",
    "        # Handle key presses\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('s'):\n",
    "            save_path = f\"camera_detection_{frame_count}.jpg\"\n",
    "            cv2.imwrite(save_path, annotated_frame)\n",
    "            print(f\"üíæ Frame saved: {save_path}\")\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"\\n‚úÖ Camera testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b2a751",
   "metadata": {},
   "source": [
    "---\n",
    "## Option 3: Test Single Image üñºÔ∏è\n",
    "\n",
    "Test the model on a single image file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a32311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üñºÔ∏è SINGLE IMAGE TESTING\n",
    "# ========================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set your image path here\n",
    "IMAGE_PATH = \"../../safespace/assets/accidents_images/test_accident.jpg\"  # Change this\n",
    "\n",
    "# Check if file exists, if not create a test from video\n",
    "if not Path(IMAGE_PATH).exists():\n",
    "    print(f\"‚ö†Ô∏è Image not found: {IMAGE_PATH}\")\n",
    "    print(\"   Extracting a frame from video for testing...\")\n",
    "    \n",
    "    test_video = \"../../safespace/assets/Video Test/test5.mp4\"\n",
    "    cap = cv2.VideoCapture(test_video)\n",
    "    if cap.isOpened():\n",
    "        # Skip to middle of video\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(cap.get(cv2.CAP_PROP_FRAME_COUNT) / 2))\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            IMAGE_PATH = \"test_frame.jpg\"\n",
    "            cv2.imwrite(IMAGE_PATH, frame)\n",
    "            print(f\"   ‚úÖ Test frame saved: {IMAGE_PATH}\")\n",
    "        cap.release()\n",
    "\n",
    "# Load and process image\n",
    "if Path(IMAGE_PATH).exists():\n",
    "    image = cv2.imread(IMAGE_PATH)\n",
    "    \n",
    "    print(f\"üì∑ Processing image: {IMAGE_PATH}\")\n",
    "    print(f\"   Size: {image.shape[1]}x{image.shape[0]}\")\n",
    "    \n",
    "    # Run detection\n",
    "    annotated_image, detections, inference_time = detect_and_annotate(\n",
    "        image, model, CONFIDENCE_THRESHOLD\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nüìä Results:\")\n",
    "    print(f\"   Inference time: {inference_time:.2f}ms\")\n",
    "    print(f\"   Detections: {len(detections)}\")\n",
    "    \n",
    "    if len(detections) > 0:\n",
    "        print(f\"\\nüö® Detected objects:\")\n",
    "        for i, (class_id, conf) in enumerate(zip(detections.class_id, detections.confidence)):\n",
    "            print(f\"   {i+1}. {model.names[class_id]} - Confidence: {conf:.2%}\")\n",
    "    \n",
    "    # Display image\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Detection Results - {len(detections)} object(s) found\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"‚ùå Could not load image: {IMAGE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd359bb",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup\n",
    "\n",
    "Run this cell to close any remaining windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb7fe4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ All windows closed\n"
     ]
    }
   ],
   "source": [
    "# Cleanup - close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n",
    "print(\"üßπ All windows closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47dface",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
